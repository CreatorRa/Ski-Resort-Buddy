#--- Swiss Data processing ---

import Pkg
Pkg.add("CSV")
Pkg.add("DataFrames")

using DataFrames
using CSV

filepath = "c:\\Users\\Carter\\OneDrive\\Documents\\KLU\\KLU Studies\\Scientific Programming\\Clean Data\\more data\\data_monthly_CH_SLF.csv"

df = CSV.read(filepath, DataFrame) #This writes the CSV into a dataframe

columns_to_drop =[
    "SCD1gt",
    "SCD10",
    "SCD20",	
    "SCD30",	
    "SCD50",	
    "SCD100",
    "HSmean_gapfill",
    "frac_gapfilled",
    "HSmax_gapfill",
    "SCD1_gapfill",
    "SCD1gt_gapfill",
    "SCD10_gapfill",
    "SCD20_gapfill",
    "SCD30_gapfill",
    "SCD50_gapfill",
    "SCD100_gapfill"
    ]
select!(df, Not(columns_to_drop)) #Removing unnecessary columns from the dataset.

dropmissing!(df, :HNsum) #This removes missing values for the Snow Depth varaible.

rename!(df, "Name" => "Region") #These functions rename the varaibles to improve readability. 
rename!(df, "HNsum" => "Snow Depth (cm)")
rename!(df, "HSmean" => "Mean Snow Depth (cm)")
rename!(df, "HSmax" => "Max Snow Depth (cm)")
rename!(df, "SCD1" => "Days where AVG Temp < 0C")
rename!(df, "year" => "Date")

insertcols!(df, 2, :Country => "Switzerland") # This code adds in a column for the country Switzerland. 

#The code below further cleans up the values by removing uncessary suffiexs making the values more uniform
df.Region = replace.(df.Region, "_CH_SLF" => "")
df.Region = replace.(df.Region, "Davos_Fluelastr_" => "Davos")
df.Region = replace.(df.Region, "St_Moritz" => "St Moritz")

#-- End of Swiss Data processing --

#-- Germany Data processing --

using DataFrames
using CSV

filepath = "C:\\Users\\Carter\\OneDrive\\Documents\\KLU\\KLU Studies\\Scientific Programming\\Clean Data\\more data\\data_monthly_DE_DWD.csv"

df2 = CSV.read(filepath, DataFrame)

# Remove unnecessary columns
columns_to_drop =[
    "SCD1gt",
    "SCD10",
    "SCD20",	
    "SCD30",	
    "SCD50",	
    "SCD100",
    "HSmean_gapfill",
    "frac_gapfilled",
    "HSmax_gapfill",
    "SCD1_gapfill",
    "SCD1gt_gapfill",
    "SCD10_gapfill",
    "SCD20_gapfill",
    "SCD30_gapfill",
    "SCD50_gapfill",
    "SCD100_gapfill"
    ]
select!(df2, Not(columns_to_drop))
dropmissing!(df2, :HNsum)

rename!(df2, "Name" => "Region")
rename!(df2, "HNsum" => "Snow Depth (cm)")
rename!(df2, "HSmean" => "Mean Snow Depth (cm)")
rename!(df2, "HSmax" => "Max Snow Depth (cm)")
rename!(df2, "SCD1" => "Days where AVG Temp < 0C")
rename!(df2, "year" => "Date")

insertcols!(df2, 2, :Country => "Germany")

df2.Region = replace.(df2.Region, "Berchtesgaden_KKst_" => "Berchtesgaden")
df2.Region = replace.(df2.Region, "Feldberg_Schwarzwald" => "Feldberg")
df2.Region = replace.(df2.Region, "Garmisch_Partenkirchen" => "Garmisch-Partenkirchen")
df2.Region = replace.(df2.Region, "Oberndorf_Neckar" => "Oberstdorf")

#column_check = ["Region"]
#Unique_values = unique(df2[:, column_check])
#println("Unique values in 'Region' column:")
#println(Unique_values)

#-- End of Germany Data processing --

#-- Austria Data Processing ---

using CSV
using Dataframe

filepath= "C:\\Users\\user\\Downloads\\data_monthly_AT_HZB.csv"
df3 = CSV.read(filepath, DataFrame)

first(df3, 5) 

colums_to_drop = [
    "SCD1gt",
    "SCD10",
    "SCD20",	
    "SCD30",	
    "SCD50",	
    "SCD100",
    "HSmean_gapfill",
    "frac_gapfilled",
    "HSmax_gapfill",
    "SCD1_gapfill",
    "SCD1gt_gapfill",
    "SCD10_gapfill",
    "SCD20_gapfill",
    "SCD30_gapfill",
    "SCD50_gapfill",
    "SCD100_gapfill"
]
select!(df3, Not(colums_to_drop))
dropmissing!(df3, :HNsum) 


rename!(df3, "Name" => "Region")
rename!(df3, "HNsum" => "Snow Depth (cm)")
rename!(df3, "HSmean" => "Mean Snow Depth (cm)")
rename!(df3, "HSmax" => "Max Snow Depth (cm)")
rename!(df3, "SCD1" => "Days where AVG Temp < 0C")
rename!(df3, "year" => "Date")

insertcols!(df3, 2, :Country => "Austria")
first(df3, 10) 

#--- End of Austria Data processing ---


#Combines the dataframes into a single data frame. 
df_combined = vcat(df, df2, df3)
println("Combined DataFrame has", nrow(df_combined),)

println(first(df_combined, 10))
println(last(df_combined, 10))

#Combine the data from ski-regions with the previous dataframes. 

filepath_combined = "c:\\Users\\Carter\\OneDrive\\Documents\\KLU\\KLU Studies\\Scientific Programming\\Clean Data\\more data\\combined_ski_data.csv"
println("Loading historical snow data from '$filepath_combined'...")
df_snow = CSV.read(filepath_combined, DataFrame)

# Load the regional weather/elevation data
filepath_regions = "c:\\Users\\Carter\\OneDrive\\Documents\\KLU\\KLU Studies\\Scientific Programming\\Clean Data\\more data\\ski-regions-data.csv"
println("Loading regional weather data from '$filepath_regions'...")
df_regions = CSV.read(filepath_regions, DataFrame)

println("Preparing data for the join...")

rename!(df_snow, "Snow Depth (cm)" => "Monthly Snow Depth (cm)")
rename!(df_regions, "Snow Depth (cm)" => "Daily Snow Depth (cm)")
# The 'Date' in the regional data is a daily date, which we'll keep.
# The 'Date' in the combined snow data is the year, so let's rename it for clarity.
rename!(df_snow, "Date" => "Year")

df_final = innerjoin(df_snow, df_regions, on = [:Region, :Country])

println("Successfully joined the data.")
println("The final DataFrame has ", nrow(df_final), " rows and ", ncol(df_final), " columns.")
println("-"^40)

println("Displaying the first 5 rows of the final joined data:")
# Use `first` to show a preview. Note the many new columns from both files.
println(first(df_final, 5))

println("\nFinal column names in the joined DataFrame:")
println(names(df_final))


#-- Exporting New Combined DataFrame to CSV file--

#This code simply writes the combined dataframe into a csv that is will be accessable for the project. 
#output_filepath = "C:\\Users\\Carter\\OneDrive\\Documents\\KLU\\KLU Studies\\Scientific Programming\\Clean Data\\more data\\Dach_resort_dataset.csv"
#CSV.write(output_filepath, Dach_resort_dataset)

#-- End of Exporting New Combined DataFrame to CSV file--


